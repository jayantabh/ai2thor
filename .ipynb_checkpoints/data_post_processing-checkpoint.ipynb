{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from nltk.corpus import wordnet as wn\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_attr_synsets = {\n",
    "    \"broken\": wn.synsets(\"broken\", pos='v')[1],\n",
    "    \"filled\": wn.synsets(\"filled\", pos='v')[2],\n",
    "    \"dirty\": wn.synsets(\"dirty\", pos='v')[0],\n",
    "    \"cooked\": wn.synsets(\"cooked\", pos='v')[2],\n",
    "    \"open\": wn.synsets(\"open\", pos='v')[0],\n",
    "    \"can be picked up\": wn.synsets(\"pick_up\", pos='v')[1],\n",
    "    \"can be moved\": wn.synsets(\"portable\")[1],\n",
    "    \"can be sliced\": wn.synsets(\"slice\", pos='v')[2],\n",
    "    \"can be used\": wn.synsets(\"use\", pos='v')[1],\n",
    "    \"on top of\": wn.synsets(\"above\")[3],\n",
    "    \"below\": wn.synsets(\"below\")[0],\n",
    "    \"to left of\": wn.synsets(\"left\")[0],\n",
    "    \"to right of\": wn.synsets(\"left\")[2],\n",
    "    \"is in-front of\": wn.synsets(\"in_front\")[0],\n",
    "    # remove after fixing this\n",
    "    \"is infront\": wn.synsets(\"in_front\")[0],\n",
    "    \"is behind\": wn.synsets(\"behind\")[2],\n",
    "    \"has\": wn.synsets(\"has\")[3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camel_case_split(str): \n",
    "    words = [[str[0]]] \n",
    "  \n",
    "    for c in str[1:]: \n",
    "        if words[-1][-1].islower() and c.isupper(): \n",
    "            words.append(list(c)) \n",
    "        else: \n",
    "            words[-1].append(c) \n",
    "  \n",
    "    return [''.join(word).lower() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_ambiguous_synsets(syns, refs):\n",
    "    max_avg_wup = 0\n",
    "    best_syn = syns[0]\n",
    "    \n",
    "    for syn in syns:\n",
    "        wup = [syn.wup_similarity(ref) for ref in refs]\n",
    "        \n",
    "        if not all(wup):\n",
    "            continue\n",
    "        \n",
    "        avg_wup = sum(wup) / len(wup)\n",
    "        \n",
    "        if avg_wup > max_avg_wup:\n",
    "            max_avg_wup = avg_wup\n",
    "            best_syn = syn\n",
    "            \n",
    "    return best_syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_unavailable_synset(unknown_word, fixed_refs, pos='n'):\n",
    "    refs = fixed_refs\n",
    "    part_words = camel_case_split(unknown_word)\n",
    "    \n",
    "    # Check if underscore separated word can be found\n",
    "    und_separated = '_'.join(part_words)\n",
    "    \n",
    "    und_separated_syns = wn.synsets(und_separated)\n",
    "    \n",
    "    if len(und_separated_syns) == 1:\n",
    "        return und_separated_syns[0]\n",
    "    elif len(und_separated_syns) > 1:\n",
    "        return resolve_ambiguous_synsets(und_separated_syns, refs)\n",
    "    \n",
    "    # Try to find closest synset from neighbouring words\n",
    "    candidates = []\n",
    "    \n",
    "    for word in part_words:\n",
    "        syns = wn.synsets(word, pos='n')\n",
    "        \n",
    "        if len(syns) == 1:\n",
    "            syn = syns[0]\n",
    "        elif len(syns) > 1:\n",
    "            syn = resolve_ambiguous_synsets(syns, refs)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        refs.append(syn)\n",
    "        \n",
    "        candidates.extend(syns)\n",
    "        \n",
    "        for hypernym in syn.hypernyms():\n",
    "            candidates.extend(hypernym.hyponyms())\n",
    "            \n",
    "    if len(candidates) == 0:\n",
    "        print(part_words, candidates)\n",
    "            \n",
    "    return resolve_ambiguous_synsets(candidates, refs)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map scene_id to image_id\n",
    "dataset_path = 'dataset/images'\n",
    "\n",
    "scene_id_map = {}\n",
    "\n",
    "image_files = [f for f in listdir(dataset_path) if isfile(join(dataset_path, f)) and f.split('.')[-1]!= 'pickle']\n",
    "\n",
    "for image_file in image_files:\n",
    "    scene_id, image_id = image_file.split('.')[0].split('_')\n",
    "    \n",
    "    scene_id = int(scene_id)\n",
    "    scene_type = None\n",
    "    \n",
    "    if scene_id < 100:\n",
    "        scene_type = 'kitchen'\n",
    "    elif scene_id < 200:\n",
    "        scene_type = 'living_room'\n",
    "    elif scene_id < 300:\n",
    "        scene_type = 'bedroom'\n",
    "    else:\n",
    "        scene_type = 'bathroom'\n",
    "    \n",
    "    scene_id_map[int(image_id)] = {\n",
    "        \"scene_id\": int(scene_id),\n",
    "        \"scene_type\": scene_type,\n",
    "        \"scene_synset\": wn.synsets(scene_type)[0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = pickle.load(open('dataset/data.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(image_data):\n",
    "    for img_id in image_data:\n",
    "        # image_id, object_ids, bounding_boxes_list, relations_map, attr_triplets, attr_doublets\n",
    "        yield (img_id, *image_data[img_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 images\n",
      "['can be picked up'] []\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d6ba22d43a7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_synsets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mattr_synsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolve_unavailable_synset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_synsets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0mattr_synsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolve_ambiguous_synsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_synsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-4bb0fbc683ec>\u001b[0m in \u001b[0;36mresolve_unavailable_synset\u001b[0;34m(unknown_word, fixed_refs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mresolve_ambiguous_synsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-d95f55307703>\u001b[0m in \u001b[0;36mresolve_ambiguous_synsets\u001b[0;34m(syns, refs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mresolve_ambiguous_synsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmax_avg_wup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mbest_syn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msyns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msyn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msyns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "data_loader = loader(image_data)\n",
    "\n",
    "image_data_json = []\n",
    "objects_json = []\n",
    "attributes_json = []\n",
    "relationship_json = []\n",
    "attribute_list = []\n",
    "relations_list = []\n",
    "synsets = {}\n",
    "\n",
    "object_uid = 0\n",
    "relationship_uid = 0\n",
    "\n",
    "object_map_internal = {}\n",
    "object_set = set()\n",
    "attribute_set = set()\n",
    "predicate_set = set()\n",
    "\n",
    "word_object_synset = wn.synsets('object')[0]\n",
    "IMAGE_HEIGHT = 1024\n",
    "IMAGE_WIDTH = IMAGE_HEIGHT\n",
    "\n",
    "for img_id, obj_ids, bb_list, rel_map, attr_map in data_loader:\n",
    "    if not img_id % 20:\n",
    "        print(f\"Processed {img_id} images\")\n",
    "    image_data_dict = {\n",
    "        \"image_id\": img_id,\n",
    "        \"url\": \"\",\n",
    "        \"width\": IMAGE_HEIGHT,\n",
    "        \"height\": IMAGE_WIDTH,\n",
    "        \"coco_id\": 0,\n",
    "        \"flickr_id\": 0\n",
    "    }\n",
    "    \n",
    "    refs = [word_object_synset, scene_id_map[img_id][\"scene_synset\"]]\n",
    "    \n",
    "    image_data_json.append(image_data_dict)\n",
    "    \n",
    "    objects_list = []\n",
    "    \n",
    "    for idx, obj_id in enumerate(obj_ids):\n",
    "        obj_name = obj_id.split('|')[0]\n",
    "        \n",
    "        object_set.add(obj_name)\n",
    "        \n",
    "        obj_synsets = wn.synsets(obj_name)\n",
    "        \n",
    "        if len(obj_synsets) == 0:\n",
    "            obj_synset = resolve_unavailable_synset(obj_name, refs)\n",
    "        elif len(obj_synsets) > 1:\n",
    "            obj_synset = resolve_ambiguous_synsets(obj_synsets, refs)\n",
    "        else:\n",
    "            obj_synset = obj_synsets[0]\n",
    "            \n",
    "        objects_list.append(\n",
    "            {\n",
    "                \"object_id\": object_uid,\n",
    "                \"x\": int(bb_list[idx][0]),\n",
    "                \"y\": int(bb_list[idx][1]),\n",
    "                \"w\": abs(int(bb_list[idx][2] - bb_list[idx][0])),\n",
    "                \"h\": abs(int(bb_list[idx][3] - bb_list[idx][1])),\n",
    "                \"names\": [obj_name],\n",
    "                \"name\": obj_name,\n",
    "                \"synsets\": [obj_synset.name()]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        synsets[obj_name] = obj_synset.name()\n",
    "        \n",
    "        object_map_internal[idx] = objects_list[-1]\n",
    "        \n",
    "        attribute_list.append(\n",
    "            {\n",
    "                \"object_id\": object_uid,\n",
    "                \"x\": int(bb_list[idx][0]),\n",
    "                \"y\": int(bb_list[idx][1]),\n",
    "                \"w\": abs(int(bb_list[idx][2] - bb_list[idx][0])),\n",
    "                \"h\": abs(int(bb_list[idx][3] - bb_list[idx][1])),\n",
    "                \"names\": [obj_name],\n",
    "                \"name\": obj_name,\n",
    "                \"synsets\": [obj_synset.name()],\n",
    "                \"attributes\": attr_map[idx]\n",
    "          }\n",
    "        )\n",
    "        \n",
    "        synsets[obj_name] = obj_synset.name()\n",
    "        \n",
    "        for attr_val in attr_map[idx]:\n",
    "            synsets[attr_val] = rel_attr_synsets[attr_val].name()\n",
    "        \n",
    "        object_uid += 1\n",
    "        \n",
    "    objects_json.append({\n",
    "        \"image_id\": img_id,\n",
    "        \"objects\": objects_list\n",
    "    })\n",
    "    \n",
    "    attributes_json.append({\n",
    "        \"image_id\": img_id,\n",
    "        \"attributes\": attribute_list\n",
    "    })\n",
    "        \n",
    "    for rel in rel_map:\n",
    "        for rel_sub_id, rel_obj_id in rel_map[rel]:\n",
    "            subject_map = object_map_internal[rel_sub_id]\n",
    "            subject_map[\"name\"] = subject_map[\"names\"][0]\n",
    "            \n",
    "            object_map = object_map_internal[rel_obj_id]\n",
    "            object_map[\"name\"] = object_map[\"names\"][0]\n",
    "            \n",
    "            relations_list.append(\n",
    "                {\n",
    "                    \"relationship_id\": relationship_uid,\n",
    "                    \"predicate\": rel,\n",
    "                    \"synsets\": [rel_attr_synsets[rel].name()],\n",
    "                    \"subject\": subject_map,\n",
    "                    \"object\": object_map,\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            predicate_set.add(rel)\n",
    "            \n",
    "            synsets[rel] = rel_attr_synsets[rel].name()\n",
    "            \n",
    "            relationship_uid += 1\n",
    "            \n",
    "    relationship_json.append({\n",
    "        \"image_id\": img_id,\n",
    "        \"relationships\": relations_list\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "dataset_path = 'dataset/'\n",
    "\n",
    "with open(os.path.join(dataset_path, 'objects.json'), 'w') as outfile:\n",
    "    json.dump(objects_json, outfile)\n",
    "    \n",
    "with open(os.path.join(dataset_path, 'relationships.json'), 'w') as outfile:\n",
    "    json.dump(relationship_json, outfile)\n",
    "    \n",
    "with open(os.path.join(dataset_path, 'image_data.json'), 'w') as outfile:\n",
    "    json.dump(image_data_json, outfile)\n",
    "\n",
    "with open(os.path.join(dataset_path, 'attributes.json'), 'w') as outfile:\n",
    "    json.dump(attributes_json, outfile)\n",
    "    \n",
    "with open(os.path.join(dataset_path, 'synsets.json'), 'w') as outfile:\n",
    "    json.dump(synsets, outfile)\n",
    "    \n",
    "with open(os.path.join(dataset_path, 'object_list.txt'), 'w') as outfile:\n",
    "    for item in object_set:\n",
    "        outfile.write(\"%s\\n\" % item)\n",
    "    \n",
    "with open(os.path.join(dataset_path, 'predicate_list.txt'), 'w') as outfile:\n",
    "    for item in predicate_set:\n",
    "        outfile.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
